\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage{pgfplots}
\usepackage[usenames,dvipsnames]{color}
\usepackage{alltt}
\usepackage{fancyvrb}
\usepackage{tcolorbox}
\usepackage{xcolor}
\definecolor{type}{HTML}{D70087}
\definecolor{number}{HTML}{D75F00}

\title{Graphics Shaders for Scaling Photorealism in Real-Time Rendering}
\author{Christopher Pane}

\begin{document}
\maketitle

\begin{abstract}
Computer graphics shaders were developed in order to test the ability to scale photorealism. Graphics shaders were developed in Unity using screen-space edge detection and color palette reduction. Vertex shaders were developed for 3D models used in tracking software. Shaders were designed to be scaled from non-photo realistic to photo realistic through a tuning parameter. The tuning parameter was controlled through the use of easing functions. Continuation of the project will include setting up face tracking and using a depth camera.
\end{abstract}

\section{Introduction}

The research done is focused on creating Unity shaders to change the style of 3D content in real time to render virtual humans in different styles ranging from realistic to non-photorealistic.

In order to emulate a non-photorealistic appearance, the following distinct qualities of non-photo realistic renderings were deduced:
\begin{enumerate}
\item Reduced color palette 
\item Thick outlines
\item Non-human features
\end{enumerate}
After examining these key qualities of non-photorealistic renderings, fragment and vertex shaders were developed to utilize screen-space textures and 3D models to scale photo-realism. Fragment shaders were used to apply post-processing effects to a screen-space texture. Vertex shaders were used to modify a mesh. Both shader implementations were combined to increase the effectiveness of the non-photo realistic illusion. Finally, the composited effects will be scaled based on a tuning parameter that ranges from 0 to 1.


\section{Fragment Shader}

\subsection{Color palette reduction}

To address the non-photorealistic quality of a reduced color palette, a coloring technique from Ordered dithering was used. Ordered dithering applies the following formula to each pixel to determine its nearest color in a limited color palette:
\[
    C_{nearest} = \frac{\lfloor N - 1 \rfloor * C + 0.5}{\lfloor N - 1 \rfloor}
\]
Where $N$ is a vector containing the number of possible colors for each color channel and $C$ is the currently sampled pixel value. By setting the original pixel color to its greyscale value, the color's of the new image now correspond to their scalar value in the range $[0, 1]$. By reinterpreting this information as texture UV coordinates along a $N x 1$ texture, custom palettes can be applied to the texture.

This color palette reduction can be used on the texture generated from a 3D scan mesh generation. When a 3D scan is used to generate a mesh, the texture generated in the can have its noise reduced and color palette reduced, effectively creating a stylized 3D model of the target. When this technique is used, edges can be applied to the mesh with an outline mesh, rather than using post-processing.

%============================================
% Reduced palette n = 2
%============================================
\begin{figure}[h!]
\centering
% \includegraphics[width=0.3\textwidth]{ChrisBlackWhite.PNG}
\includegraphics[height=5cm]{ChrisBlackWhite.PNG}
\caption{\label{fig:BlackWhite}Color palette reduction applied on a greyscale image with N = 2.}
\end{figure}

%============================================
% Display of sampler
%============================================
\begin{figure}[h]

    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.5\linewidth, height=2cm]{CustomSampler.PNG} 
        \caption{Custom sampler of size $N x 1$}
        \label{fig:CustomSampler}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[height=5cm]{ChrisCustomSampler.PNG}
        \caption{\ref{fig:BlackWhite} with pixel values as UV coordinates to sample \ref{fig:CustomSampler}}
        \label{fig:subim2}
    \end{subfigure}

    \caption{Using a greyscale image to map to a new color pallete}
    \label{fig:CustomSamplerDemo}
\end{figure}

\subsection{Screen-space outline detection}

To apply thick outlines to a screen-space texture, a screen-space edge detection used. After consider a variety of edge detection algorithms, it was decided that using a canny edge-detection algorithm would be effective in creating the thick borders. 

To maintain the performance of the real-time render, only 2 passes of the 5 pass canny edge-detection were used.
\begin{enumerate}
\item Apply a Gaussian filter to blur the texture and reduce noise.
\item Convolve the texture with the Sobel operator to calculate the gradients.
\end{enumerate}

After calculating the edges of the base image, the edge detection texture can be composited with the base texture to apply edges to the image. The magnitude determined by the Sobel operator can be used to $step$ the value based on a threshold, which creates sharp edges.

%============================================
% Comparison of edge detection methods
%============================================
\begin{figure}[!htb]

    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[height=5cm]{EdgeNoBlur.png}
        \caption{Edge detection on the base image.}
        \label{fig:EdgeNoBlur}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[height=5cm]{EdgeBlur.png} 
        \caption{Edge detection on the Gaussian blurred image.}
        \label{fig:EdgeBlur}
    \end{subfigure}

    \caption{Comparison of edge detection on a blurred and non-blurred image. Blurring the base image before edge detection considerably reduces noise, as well as enhances only the most prominent edges.}
    \label{fig:EdgeComparison}
\end{figure}

\section{Vertex Shader}

\subsection{Axis-bound growth}

Vertex shaders were used to aid in creating non-photorealistic imagery. It was decided that a technique to adjust the realism of a 3D model was to scale the vertices away from the object origin. However, simply scaling the vertices by a scalar would not aid in the illusion of controlled realism.

The vertex shader was designed to scale each vertex based on a per-vertex encoded vector describing the axis on which the vertex scaling should occur. This information was encoded into the per-vertex color attribute and decoded in the shader to determine how a vertex should be scaled.

This technique takes advantage of the per-vertex attribute $\text{color}$ which typically contains color information from the range $[0, 1]$. By treating this color value as a vector, each vertex can be scaled along an axis at a certain weight. The color value encoded in the per-vertex color was the vector information containing the axis to scale a vertex along.


Given a vertex $V$ positioned at $P_i = \begin{pmatrix}A & B & C\end{pmatrix}$, its per-vertex color $C = \begin{pmatrix}R & G & B\end{pmatrix}$, and a scaling factor $S$, the following formula can be used to determine the vertex's final scaled position:
\[
    P_f = P_i - P_iC + P_iCS = P_i(1 + B(S - 1))
\]

Using this technique, the 3D models used needs to be prepared with:
\begin{itemize}
\item Growth axis color information painted on
\item A sensible location of the object origin
\end{itemize}

%============================================
% Bunny Ears grahpic
%============================================
\begin{figure}[h]

    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[height=5cm]{BunnyEarsTextured.PNG} 
        \caption{A textured mesh representing bunny ears, a nose, and whiskers}
        \label{fig:BunnyEarsTextured}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[height=5cm]{BunnyEarsVertexColors.PNG}
        \caption{The same mesh colored with the per-vertex colors corresponding to its desired growth axis}
        \label{fig:BunnyEarsVertexColors}
    \end{subfigure}

    \caption{An example textured mesh and its underlying per-vertex colors representing vector information.}
    \label{fig:BunnyEarsMesh}
\end{figure}

From the vertex colors shown in \ref{fig:BunnyEarsVertexColors}, each per-vertex color $\begin{pmatrix}R & G & B\end{pmatrix}$ with values in the range $[0, 1]$ directly correspond to the vector $\begin{pmatrix}X & Y & Z\end{pmatrix}$ which represents the weights of how much to scale each axis. In the case for Unity, red corresponds to $X$, green corresponds to $Y$, and blue corresponds to $Z$.
The advantage to storing growth axis information in the per-vertex color attribute instead of in the game engine is performance. By having the information available in the mesh, the GPU never has to interact with the CPU about which axis for a vertex to scale along. Additionally, another advantage is the ease of encoding this information in a 3D modeling software. The process is very similar to weight painting, which makes the process intuitive to an experienced modeler.

\begin{figure}[h]
\centering
\includegraphics[height=5cm]{BunnyEarsScaled65.PNG}
\caption{\label{fig:BunnyEarsScaled65}The \ref{fig:BunnyEarsMesh} mesh with $S = 0.65$ and the original size outlines present.}
\end{figure}

In figure \ref{fig:BunnyEarsScaled65}, it can be seen that parts of the mesh colored red only scale along the $X$ axis, while the parts colored light-green scale along the $Y$ axis entirely and slightly along the $X$ axis. The outlines of the original unscaled mesh remain to help visualize the scaling. Additionally, it can be seen that the nose mesh scales uniformly along each axis. This is caused by the per-vertex color evaluating to $\begin{pmatrix}1 & 1 & 1\end{pmatrix}$, or white.

\subsection{Mesh outline}

To apply an outline to a 3D model in world-space, a copy of the mesh with its normals flipped was created, scaled up slightly, and removed from the lighting pipeline.

\section{Tuning Parameter}
\subsection{Connecting the Tuning Parameter}
Although each shader would be controlled by a single tuning parameter, each shader would use the parameter differently.

\subsubsection{Color Palette Reduction}
The tuning parameter mixes the reduced color palette texture with the base image texture. Effectively, this performs a linear interpolation between the base texture and the reduced texture based on the tuning parameter.
\begin{tcolorbox}
\begin{alltt}
out_color = \textcolor{type}{mix}(base, reduced, tuning_parameter)
\end{alltt}
\end{tcolorbox}

\subsubsection{Screen-space outline detection}
The tuning parameter scales the cut-off value threshold for whether a pixel identified by the canny-edge detection algorithm is a strong edge. 
\begin{tcolorbox}
\begin{alltt}
out_color = \textcolor{type}{step}(tuning_parameter * threshold, \textcolor{type}{vec3}(edge_value))
\end{alltt}
\end{tcolorbox}

\subsubsection{Axis-bound growth}
The tuning parameter serves as the cut-off value for whether a pixel identified by the canny-edge detection algorithm is a strong edge. 
\begin{tcolorbox}
\begin{alltt}
\textcolor{type}{vec3} pos = vertex.position
\textcolor{type}{vec3} basis = vertex.color
out_position = pos * (\textcolor{number}{1} + basis * (tuning_parameter - \textcolor{number}{1}))
\end{alltt}
\end{tcolorbox}

\subsubsection{Mesh outline}
The tuning parameter simply serves as a scaling factor of the vertex's position.
\begin{tcolorbox}
\begin{alltt}
out_position = vertex.position * max_scale * tuning_parameter
\end{alltt}
\end{tcolorbox}

\subsection{Controlling the Tuning Parameter}
It could be seen that some visual effects should not scale linearly with the tuning parameter. For example, it may be preferred that the outline effect remain until very low values of the tuning parameter. In order to allow for more flexibility of controlling the tuning parameter, easing functions were added to the shaders.

In \ref{fig:tuningexample}, the intensity of the effects can be seen at varying levels of the tuning parameter. The color palette reduction effect scales linearly with the tuning parameter and the screen-space edge detection effect scales based on $\sqrt{x}$. Because the edge detection scales with $\sqrt{x}$, the edge effect remains more prominent at lower tuning parameter values.

%============================================
% Graph of sqrt X
%============================================
\begin{figure}[h]
\centering
\begin{tikzpicture}
    \begin{axis}[
        samples=200,
        xmin=0, xmax=1,
        ymin=0, ymax=1,
        xlabel={Tuning Parameter $x$},
        ylabel={$\sqrt{x}$}
    ]
        \addplot [mark=none, domain={0:1}, restrict y to domain={0:1}] {x^(1/2)};
    \end{axis}
\end{tikzpicture}
\caption{The tuning parameter after the easing function $\sqrt{x}$ was applied.}
\end{figure}

%============================================
% Triple display of tuning values
%============================================
\begin{figure}
\minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{TuningFull.png} 
    \caption{1.0}
    \label{fig:tuningfull}
\endminipage\hfill
\minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{TuningHalf.png}
    \caption{0.5}
    \label{fig:tuninghalf}
\endminipage\hfill
\minipage{0.32\textwidth}
    \includegraphics[width=\linewidth]{TuningQuarter.png}
    \caption{0.25}
    \label{fig:tuningquarter}
\endminipage
\caption{\label{fig:tuningexample}Original image with different tuning parameter values.}
\end{figure}

\section{Further Continuation}
Further continuation of this software research project would consist of integrating a real-time tracking software into the Unity project to allow 3D models to be tracked to the position and rotation of a person's features. This would be required to enable the use of the vertex shader based effects, such as mesh outlines and axis-bound growth. 

Additionally, the use of a depth camera would allow more features to be applied, such as distance based rendering. This could be used to seperate the focus target with the background and aid in the non-photorealistic render.

\section*{Acknowledgement}
Dr. Salam Daher

\begin{thebibliography}{9}
\bibitem{thebookofshaders}
    The Book of Shaders: https://thebookofshaders.com
\end{thebibliography}

\end{document}
